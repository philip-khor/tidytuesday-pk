---
title: "R Notebook"
output: 
  github_document:
    toc: true
  html_document: 
    toc: true
    keep_md: true
---

# Load the data

Generate a time series of counts of tweets 

```{r, setup}
library(lubridate)
library(tsibble)
library(forecast)
library(tidyverse)
library(cowplot)
library(rlang)
library(magrittr)
library(glue)
library(hrbrthemes)
library(here)
```

# Create a time series of counts of tweets per day 

- Check for gaps using `tsibble`

```{r, data, cache=TRUE}
load_data <- function(x){
  glue("https://github.com/rfordatascience/tidytuesday/raw/master/data/2019/2019-01-01/{x}_tweets.rds") %>% 
  url(description = .) %>% 
  readRDS() %>% 
  list(ts = count(., date = round_date(created_at, "day")), 
       start_date = week(round_date(min(.$created_at), 
                                    unit = "week")))
}

ls_data <- map(c(TidyTuesday = "tidytuesday", 
              rstats = "rstats"), 
            load_data)

str(ls_data, max.level = 2)
```

# Check for gaps in the time series 

```{r}
check_for_gaps <- . %>% 
  pluck(2) %>% 
  mutate(date = as_date(date)) %>% 
  as_tsibble(regular = TRUE) %>% 
  count_gaps(.full = TRUE)

map_df(ls_data, check_for_gaps, .id = "hashtag")
```

```{r}
# fill gaps with 0s
filling_gaps <- function(x){ 
  x[[2]] <- x[[2]] %>% 
    mutate(date = as_date(date)) %>%
    as_tsibble(regular = TRUE) %>% 
    fill_gaps(n = 0L) 
  return(x)
}
data <- map(ls_data, filling_gaps) 
```

```{r}

# start at first day of week
polar_plot <- function(.x, .y) {
  .x$ts %>% 
    pull(n) %>% 
    ts(frequency = 7, start = c(1, .x$created_at)) %>% 
    ggseasonplot(main = glue("#{.y} tweets"), 
                 continuous = TRUE, 
                 polar = TRUE) + theme_minimal() + 
    labs(color = "week")
}

map2(ls_data, names(ls_data), polar_plot) %>% 
  plot_grid(plotlist = .)
```

# Count of tweets over time 
```{r}
str(ls_data, max.level = 2)
```


```{r, countline}
bind_rows(ls_data$TidyTuesday[[2]], ls_data$rstats[[2]], .id = "hashtag") %>% 
  mutate(hashtag = case_when(hashtag == 1 ~ "TidyTuesday", 
                             TRUE ~ "rstats")) %>% 
  ggplot(aes(x = date, y = n, color = hashtag)) + geom_line() + theme_minimal()
```

# Manual random check on suspicious looking screen names 

Some are cleaning services, city councils/community volunteers, social media influencers sharing tweets on how to be tidy, and community R accounts that are unlikely to amount to user submissions. I manually look up some screen names to be sure. 

To filter for user submissions, I use the following (rather crude) rules: 

 - People posting from the [Buffer](https://buffer.com/) app are filtered out. Buffer is a social media management app - it's unlikely that participants are posting from Buffer. 

```{r}
communities <- c("RLadiesELansing", "PodsProgram", "WeAreRLadies", "RLadiesLondon", "R4DScommunity",  "thomas_mock"
                 )
others <- c("AirdrieExchange", "tidyyourworld", "SavvySpaceSolut", "emfriesen", "stevie_t13", "EjayOnline",
            "worldcattitude", "SummitAppliance", "BellaBooDC", "RouteMediaUK", "London__Digital", 
            "AdoptionsWLove", "_Mindmassage", "_VictoriaPlace", "BonnieC919", "EnglishHomeTeam", "fakrogb", "town_of_pc", 
            "racinggreenmids", "FitKitchens", "GlowExpressWash", "GlitterOnADime", "HomerHelper", "leafinnovations",
            "LivingSimplyLR", "BooksForOrphans", "BishopSport", "BroomsInBloom", "CDSChester", "CMwaste2007", 
            "CountrywithKim", "PMPUtilities", "Sarah_C_Church", "SugarLesswithC", "SportandiAndres",
            "Shortie_Amazing", "silicasun", "sirfrancishill", "steph4smith", "rubyslifestyle", 
            "PinkGiraffe_", "RogersUptown", "OrganisedWell", "StanleyWingHK", "RGTB", "OrganisingNinja", 
            "naturalstclean", "Nightingale_NTA", "PurfectSoluxion", "SimsLifeBlog", "ThomsonPlumbing")
screen_filter <- c(communities, others)

tidied <- ls_data$TidyTuesday[[1]] %>% 
  unnest(media_type) %>% 
  mutate(week = ymd("2018-04-02") %--% as_date(created_at)/dweeks(1), 
         source_filter = !((screen_name %in% screen_filter)  | (source == "Buffer")),
         
         # does help that most participants properly thread their submissions 
         # so it's unlikely we get a submission that's a reply to another tweet
         # we want to count each participant once anyway, so ...
         reply_filter = is.na(reply_to_screen_name),
         
         # this would (hopefully) remove most tweets discussing the project
         # in favour of **actual** submissions
         photos_only = media_type %in% "photo") 
```

# Compiling a leaderboard 

From this we can compile a leaderboard on who makes the most #TidyTuesday submissions. 

```{r}
leaderboard <- tidied %>% 
  filter(source_filter & reply_filter & photos_only) %>% 
  count(screen_name, week) %>% 
  rename(tweets = n ) %>% 
  mutate(tweets = na_if(x = tweets, y = 0)) %>% # treat weeks where 0 as NAs 
  # average of 0s not useful here
  group_by(screen_name) %>% 
  summarise(min = min(tweets, na.rm = TRUE), 
            max = max(tweets, na.rm = TRUE), 
            mean = mean(tweets, na.rm = TRUE), 
            sum = sum(tweets, na.rm = TRUE)) %>% 
  arrange(desc(sum)) 

head(leaderboard, 10) %>% 
  select(screen_name, sum) %>% 
  knitr::kable()
```

## Save and visualise the leaderboard 

```{r}
write_csv(leaderboard, path = here::here("2019", "w1_2019", "data", "tt_leaderboard.csv"))

ggplot(leaderboard, aes(x = sum)) + 
  geom_histogram() + 
  labs(title = "Distribution of participants' #TidyTuesday submissions", 
       subtitle = "Approximate") + theme_ipsum()
```

```{r}
tidied_ts <- tidied %>% 
  filter(source_filter & reply_filter & photos_only) %>% 
  count(date = round_date(created_at, "day")) %>% 
  mutate(id = "id", 
         date = as_date(date)) %>%
  as_tsibble(key = id(id), index = date, regular = TRUE) 

tidied_ts %>% 
  count_gaps()

tidied_ts %>% 
  fill_gaps(n = 0L) %>% 
  pull(n) %>% 
  ts(frequency = 7, start = c(1, 3)) %>% 
  ggseasonplot(main = "#TidyTuesday 'submissions'", 
               continuous = TRUE, polar = TRUE) + 
  labs(color = "week", 
       subtitle = "Most Twitter #TidyTuesday 'submissions' occur on Tue and Wed", 
       caption = "Submissions defined as tweets containing photos that are not replies to other tweets. \
       Tweets posted by non-R users, via the Buffer app and R community accounts excluded.") +
  theme_minimal()
```

This contrasts with the conclusion we made at the start. If we had not filtered for submissions, we may have been misled into thiking that most people post their #TidyTuesday work at the end of the week, and even on Mondays. 

```{r}
count_hourly_tweets <- . %>% 
  mutate(date = round_date(created_at, unit = "hour")) %>% 
  group_by(date) %>% 
  summarise(n = n()) %>% 
  as_tsibble(index = date, regular = TRUE) %>% 
  fill_gaps(n = 0L) %>% 
  index_by(hour = hour(date)) %>%
  summarise(n = sum(n))

all_tt <- tidied %>% count_hourly_tweets()

filtered_tt <- tidied %>% 
  filter(source_filter & reply_filter & photos_only) %>%
  count_hourly_tweets()

ggplot() + geom_line(all_tt, mapping = aes(x = hour, y = n), color = 'grey') + 
  geom_line(filtered_tt, mapping = aes(x = hour, y = n), color = 'blue') + 
  labs(title = "Most of the filtered tweets are posted between 11am and 9pm", 
       subtitle = "'Submissions' are more evenly distributed through the day") + 
  theme_ipsum()

```

